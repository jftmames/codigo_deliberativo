import os
import json
import streamlit as st
import openai
import pandas as pd

# 0. ConfiguraciÃ³n de pÃ¡gina
st.set_page_config(
    page_title="CÃ³digo Deliberativo Educativo",
    page_icon="ğŸ§ ",
    layout="wide"
)

# 1. Barra lateral â€“ Ajustes
st.sidebar.header("ğŸ”§ Ajustes")
st.sidebar.markdown(
    """
    **Perfil**  
    Elige tu nivel de profundidad y guiado en la indagaciÃ³n.
    """
)
mode = st.sidebar.selectbox(
    "Perfil del usuario",
    ["Asistido (bÃ¡sico)", "Guiado (intermedio)", "Exploratorio (avanzado)"]
)
st.sidebar.markdown("---")
st.sidebar.markdown(
    """
    **API Key**  
    AsegÃºrate de haber configurado tu variable de entorno  
    `OPENAI_API_KEY` antes de desplegar.
    """,
    help="Se usa para autenticar con OpenAI."
)

# 2. TÃ­tulo principal
st.title("ğŸ§  CÃ³digo Deliberativo para Pensamiento CrÃ­tico")
st.markdown(
    """
    Esta aplicaciÃ³n te guÃ­a paso a paso para **estructurar**, **explorar** y **evaluar**
    procesos de indagaciÃ³n crÃ­ticos, usando un modelo de IA deliberativa.
    """
)

# 3. Entrada de la pregunta con ejemplos
st.header("1. Define tu pregunta raÃ­z")
example_questions = [
    "Â¿Es Ã©tico el uso de IA en diagnÃ³sticos mÃ©dicos?",
    "Â¿DeberÃ­an las redes sociales regular cierto tipo de contenido?",
    "Â¿CÃ³mo impacta la automatizaciÃ³n en el mercado laboral juvenil?",
    "Â¿Es sostenible el modelo econÃ³mico actual?",
    "Â¿Debe implementarse la renta bÃ¡sica universal?"
]
st.markdown("ğŸ” **Selecciona un ejemplo** o escribe tu propia pregunta:")
selected_example = st.selectbox("Ejemplos de preguntas", ["â€” Ninguno â€”"] + example_questions)
if selected_example != "â€” Ninguno â€”":
    root_question = selected_example
    st.markdown(f"**Pregunta seleccionada:** {root_question}")
else:
    root_question = st.text_input(
        "Escribe tu pregunta aquÃ­",
        placeholder="Ej. Â¿Es Ã©tico el uso de IA en diagnÃ³sticos mÃ©dicos?",
        help="Puedes usar uno de los ejemplos o escribir la tuya."
    )
if not root_question:
    st.warning("ğŸ›ˆ Necesitamos una pregunta raÃ­z para continuar.")
    st.stop()

# Prepara OpenAI
openai.api_key = os.getenv("OPENAI_API_KEY")
def chat(messages, max_tokens=500):
    return openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=0.7,
        max_tokens=max_tokens,
    )

# 4. GeneraciÃ³n de Subpreguntas
st.header("2. GeneraciÃ³n de Subpreguntas ğŸ”")
with st.spinner("Creando Ã¡rbol de indagaciÃ³nâ€¦"):
    inquiry_prompt = (
        "Eres un generador de subpreguntas para fomentar el pensamiento crÃ­tico.\n"
        f"Pregunta raÃ­z: â€œ{root_question}â€\n"
        f"Perfil: {mode}\n\n"
        "1. Identifica 3â€“5 subpreguntas relevantes.\n"
        "2. OrganÃ­zalas en una estructura jerÃ¡rquica.\n"
        "Responde solo con JSON:\n"
        '{ "node": "â€¦", "children": [ â€¦ ] }'
    )
    resp1 = chat([{"role": "system", "content": inquiry_prompt}], max_tokens=600)
    try:
        inquiry_tree = json.loads(resp1.choices[0].message.content)
    except Exception:
        st.error("âŒ No se pudo interpretar la respuesta del modelo.")
        st.stop()

# VisualizaciÃ³n con Graphviz
st.subheader("Ãrbol de IndagaciÃ³n")
def build_dot(node):
    edges = ""
    label = node.get("node", "<sin etiqueta>")
    for child in node.get("children", []):
        c_label = child.get("node", "<sin etiqueta>")
        edges += f"\"{label}\" -> \"{c_label}\";\n"
        edges += build_dot(child)
    return edges

root = inquiry_tree[0] if isinstance(inquiry_tree, list) else inquiry_tree
dot = f"digraph G {{\n{build_dot(root)}}}"
st.graphviz_chart(dot, use_container_width=True)

# Expander con lista de subpreguntas
st.subheader("ğŸ“‹ Ver todas las subpreguntas")
with st.expander("Mostrar subpreguntas en formato de lista"):
    def render_list(node, indent=0):
        st.markdown(" " * indent * 2 + f"- **{node.get('node', '<sin tÃ­tulo>')}**")
        for c in node.get("children", []):
            render_list(c, indent + 1)
    render_list(root)

# 5. Respuestas Contextualizadas
st.header("3. Respuestas Contextualizadas ğŸ’¬")
responses = {}
def recurse_responses(node):
    prompt = (
        "Eres un Generador Contextual de IA deliberativa.\n"
        f"Nodo: â€œ{node['node']}â€\nPerfil: {mode}\n\n"
        "Proporciona tres respuestas:\n"
        "1. Perspectiva Ã©tica.\n"
        "2. Perspectiva histÃ³rica.\n"
        "3. Perspectiva crÃ­tica.\n\n"
        "Responde solo con JSON:\n"
        '{"node":"...","responses":[{"label":"Ã‰tica","text":"..."},...]}'
    )
    r = chat([{"role": "system", "content": prompt}], max_tokens=800)
    try:
        data = json.loads(r.choices[0].message.content)
    except Exception:
        data = {"responses": []}
    responses[node["node"]] = data.get("responses", [])
    for c in node.get("children", []):
        recurse_responses(c)

with st.spinner("Generando respuestasâ€¦"):
    recurse_responses(root)
for node, resp_list in responses.items():
    st.markdown(f"**{node}**")
    for item in resp_list:
        st.write(f"- **{item['label']}**: {item['text']}")

# 6. ReformulaciÃ³n de Foco
st.header("4. ReformulaciÃ³n de Foco ğŸ”„")
focus_suggestions = []
with st.spinner("Analizando reformulacionesâ€¦"):
    prompt2 = (
        "Eres un Motor de DiÃ¡logo Adaptativo.\n"
        f"Ãrbol (JSON): {json.dumps(inquiry_tree, ensure_ascii=False)}\n"
        f"Perfil: {mode}\n\n"
        "Si hay ambigÃ¼edad, sugiere hasta 2 reformulaciones de la pregunta.\n"
        "Responde solo con JSON de lista:\n"
        '[{"original":"â€¦","suggestions":["â€¦","â€¦"]},â€¦]'
    )
    r2 = chat([{"role": "system", "content": prompt2}], max_tokens=300)
    try:
        focus_suggestions = json.loads(r2.choices[0].message.content)
    except Exception:
        focus_suggestions = []

if focus_suggestions:
    for s in focus_suggestions:
        st.info(f"> **Original:** {s.get('original')}")
        for sug in s.get("suggestions", []):
            st.write(f"- {sug}")
else:
    st.success("No se necesitan reformulaciones.")

# 7. Ãndice EEE & descarga
st.header("5. Ãndice de Equilibrio ErotÃ©tico (EEE) & Informe ğŸ“¥")
log = {
    "root": root_question,
    "inquiry": inquiry_tree,
    "responses": responses,
    "focus": focus_suggestions,
}
def calc_eee(root_node, responses, focus):
    def depth(n):
        return 1 + max((depth(c) for c in n.get("children", [])), default=0)
    prof = depth(root_node)
    avg_resp = sum(len(v) for v in responses.values()) / max(len(responses),1)
    rev = len(focus)
    d, p, r = min(prof/5,1), min(avg_resp/3,1), min(rev/2,1)
    return round((d+p+r)/3, 2)

eee = calc_eee(root, responses, focus_suggestions)
st.metric("EEE", f"{eee} / 1.00")

# Extras: modos de visualizaciÃ³n y mÃ©tricas detalladas
st.header("6. Modos de VisualizaciÃ³n y MÃ©tricas Detalladas âš™ï¸")
view_mode = st.radio(
    "Modo de vista del Ã¡rbol:",
    ("Grafo (predeterminado)", "Listado anidado")
)
if view_mode == "Listado anidado":
    render_list(root)
else:
    st.graphviz_chart(dot, use_container_width=True)

def calc_eee_components(root_node, responses, focus):
    def depth(n):
        return 1 + max((depth(c) for c in n.get("children", [])), default=0)
    prof = depth(root_node)
    avg_resp = sum(len(v) for v in responses.values()) / max(len(responses),1)
    rev = len(focus)
    return {
        "Profundidad": round(min(prof/5,1),2),
        "Pluralidad": round(min(avg_resp/3,1),2),
        "Reversibilidad": round(min(rev/2,1),2)
    }

components = calc_eee_components(root, responses, focus_suggestions)
components["EEE"] = eee
st.subheader("MÃ©tricas de Calidad EpistÃ©mica")
st.table(components.items())
st.subheader("GrÃ¡fico de Componentes del EEE")
df = pd.DataFrame.from_dict(components, orient="index", columns=["Valor"]).drop("EEE")
st.bar_chart(df)

# BotÃ³n de descarga y celebraciÃ³n
st.download_button(
    label="â¬‡ï¸ Descargar informe",
    data=json.dumps(log, ensure_ascii=False, indent=2),
    file_name="informe_deliberativo.json",
    mime="application/json"
)
st.balloons()

import streamlit.components.v1 as components

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# HTML enriquecido del informe deliberativo
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”

st.header("ğŸ“„ Informe Detallado en Formato HTML")

def render_html_tree(node):
    html = f"<li><strong>{node.get('node','')}</strong>"
    children = node.get("children", [])
    if children:
        html += "<ul>"
        for c in children:
            html += render_html_tree(c)
        html += "</ul>"
    html += "</li>"
    return html

def generate_html_report(log, eee_val, components):
    root = log["inquiry"][0] if isinstance(log["inquiry"], list) else log["inquiry"]
    html = f"""
    <html>
    <head>
    <style>
      body {{ font-family: 'Segoe UI', sans-serif; line-height: 1.6; padding: 20px; color: #333; }}
      h2 {{ color: #1f4e79; border-bottom: 1px solid #ccc; padding-bottom: 0.3em; }}
      ul {{ list-style-type: disc; margin-left: 1em; }}
      .responses {{ margin-left: 1em; }}
      table {{ border-collapse: collapse; width: 60%; margin-top: 10px; }}
      th, td {{ border: 1px solid #ddd; padding: 8px; }}
      th {{ background-color: #f2f2f2; text-align: left; }}
      .section {{ margin-bottom: 2em; }}
    </style>
    </head>
    <body>
      <h1>Informe de IndagaciÃ³n CrÃ­tica - CÃ³digo Deliberativo</h1>

      <div class="section">
        <h2>1. Pregunta RaÃ­z</h2>
        <p>{log['root']}</p>
      </div>

      <div class="section">
        <h2>2. Ãrbol de Subpreguntas</h2>
        <ul>{render_html_tree(root)}</ul>
      </div>

      <div class="section">
        <h2>3. Respuestas Contextualizadas</h2>
    """

    for nodo, resp_list in log["responses"].items():
        html += f'<div class="responses"><strong>{nodo}</strong><ul>'
        for r in resp_list:
            html += f'<li><em>{r["label"]}:</em> {r["text"]}</li>'
        html += "</ul></div>"

    html += """
      </div>
      <div class="section">
        <h2>4. Reformulaciones Sugeridas</h2>
    """

    if log["focus"]:
        html += "<ul>"
        for f in log["focus"]:
            html += f'<li><strong>{f["original"]}</strong><ul>'
            for s in f["suggestions"]:
                html += f"<li>{s}</li>"
            html += "</ul></li>"
        html += "</ul>"
    else:
        html += "<p>No se sugirieron reformulaciones.</p>"

    html += """
      </div>
      <div class="section">
        <h2>5. MÃ©tricas EpistÃ©micas (EEE)</h2>
        <table>
          <tr><th>DimensiÃ³n</th><th>Valor</th></tr>
    """
    for k, v in components.items():
        html += f"<tr><td>{k}</td><td>{v}</td></tr>"
    html += f"""
          <tr><td><strong>EEE Global</strong></td><td><strong>{eee_val}</strong></td></tr>
        </table>
      </div>
    </body>
    </html>
    """
    return html

# Calculamos mÃ©tricas desglosadas para la tabla
eee_components = calc_eee_components(root, responses, focus_suggestions)
html_final = generate_html_report(log, eee, eee_components)
components.html(html_final, height=800, scrolling=True)
